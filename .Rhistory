D<-data('mtcars')
str(D)
summary('mtcars')
str('mtcars')
str(mtcars)
LC=lm(mtcars$mpg~mtcars$wt)
LC
x <- c(8.58, 10.46, 9.01, 9.64, 8.86)
xb=(x-mean(x))/sd(x)
xb
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
LC=lm(y~x)
LC
LC=lm(x~y)
LC
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y<-zeros(length(x))
y<-zero(length(x))
y<-0*x
y
LC=lm(y~offset(0*x))
LC
LC=lm(x~1)
LC
x <- c(0.18, -1.54, 0.42, 0.95)
w <- c(2, 1, 3, 1)
z <- x*w
mean(z)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
b1 <- cor(x,y)*sd(y)/sd(x)
b0 <- mean(y) - b1 * mean(x)
b0
x
x-mean(x)
I(x-mean(x))
library(UsingR)
install.packages("UsingR")
data(diamond)
library(UsingR)
install.packages("UsingR")
install.packages("C:/Users/Administrator/Downloads/UsingR_2.0-6.tar.gz", repos = NULL, type = "source")
library(UsingR)
install.packages("usingR")
install.packages("UsingR")
install.packages("C:/Users/Administrator/Downloads/UsingR_2.0-6.zip", repos = NULL, type = "win.binary")
library(UsingR)
install.packages("HistData")
data(diamond)
?I
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
install.packages("AppliedPredictiveModeling")
install.packages("AppliedPredictiveModeling")
AppliedPredictiveModeling
install.packages("AppliedPredictiveModeling")
install.packages("C:/Users/Administrator/Downloads/AppliedPredictiveModeling_1.1-7.tar.gz", repos = NULL, type = "source")
install.packages("C:/Users/Administrator/Downloads/AppliedPredictiveModeling_1.1-7.zip", repos = NULL, type = "win.binary")
AppliedPredictiveModeling
library(AppliedPredictiveModeling)
install.packages("AppliedPredictiveModeling")
install.packages("AppliedPredictiveModeling")
install.packages("devtools")
options(repos="https://CRAN.R-project.org")
library(AppliedPredictiveModeling)
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
install.packages("caret")
inTrain <- createDataPartition(y = segmentationOriginal$Case, p = 0.6,
list = FALSE) # 60% training
training <- segmentationOriginal[inTrain, ]
testing <- segmentationOriginal[-inTrain, ]
# 2. Set the seed to 125 and fit a CART model with the rpart method using all predictor variables and default caret settings. (The outcome class is contained in a factor variable called Class with levels "PS" for poorly segmented and "WS" for well segmented.)
set.seed(125)
modFit <- train(Class ~ ., method = "rpart", data = training)
data(segmentationOriginal)
inTrain <- createDataPartition(y = segmentationOriginal$Case, p = 0.6,
list = FALSE) # 60% training
training <- segmentationOriginal[inTrain, ]
testing <- segmentationOriginal[-inTrain, ]
library(caret)
library(ggplot2)
library(lattice)
library(caret)
inTrain <- createDataPartition(y = segmentationOriginal$Case, p = 0.6,
list = FALSE) # 60% training
training <- segmentationOriginal[inTrain, ]
testing <- segmentationOriginal[-inTrain, ]
set.seed(125)
modFit <- train(Class ~ ., method = "rpart", data = training)
install.packages("e1071")
str(segmentationOriginal)
colnames(segmentationOriginal)
segmentationOriginal$Case
segmentationOriginal$Class
?rpart
install.packages("pgmm")
library(pgmm)
data(olive)
olive = olive[,-1]
newdata = as.data.frame(t(colMeans(olive)))
modolive <- train(Area ~ ., method = "rpart", data = olive)
predict(modolive, newdata = newdata)
modolive
library(rpart.plot)
fancyRpartPlot(modFit$finalModel)
library(rpart.plot)
fancyRpartPlot(modolive)
install.packages("rpart.plot")
library(rpart.plot)
fancyRpartPlot(modolive)
library(ggplot2)
library(rpart.plot)
fancyRpartPlot(modolive)
library(caret)
library(rpart.plot)
fancyRpartPlot(modolive)
library(AppliedPredictiveModeling)
library(rpart.plot)
fancyRpartPlot(modolive)
library(hplot)
install.packages("hplot")
library(rpart)
library(rpart.plot)
fancyRpartPlot(modolive)
library(rpart.plot)
rpart.plot(modolive)
library(RColorBrewer)
library(rattle)
install.packages("rattle")
modolive <- train(Area ~ ., method = "rpart", data = olive)
library(pgmm)
data(olive)
olive = olive[, -1]
newdata = as.data.frame(t(colMeans(olive)))
modolive <- train(Area ~ ., method = "rpart", data = olive)
rpart.plot(modolive$results)
rpart.plot(modolive)
str(modolive)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
suppressMessages(library(caret))
# 1. Subset the data to a training set and testing set based on the Case variable in the data set.
inTrain <- createDataPartition(y = segmentationOriginal$Case, p = 0.6,
list = FALSE) # 60% training
training <- segmentationOriginal[inTrain, ]
testing <- segmentationOriginal[-inTrain, ]
# 2. Set the seed to 125 and fit a CART model with the rpart method using all predictor variables and default caret settings. (The outcome class is contained in a factor variable called Class with levels "PS" for poorly segmented and "WS" for well segmented.)
set.seed(125)
modFit <- train(Class ~ ., method = "rpart", data = training)
modFit$finalModel
modolive$finalModel
rpart.plot(modolive$finalModel)
head(olive#Area)
%
head(olive$Area)
str(olive$Area)
summary(olive$Area)
qplot(olive$Area)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
install.packages("ElemStatLearn")
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
missClass = function(values, prediction){sum(((prediction > 0.5) * 1) != values) / length(values)}
set.seed(13234)
modelSA <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl,
data = trainSA, method = "glm", family = "binomial")
missClass(testSA$chd, predict(modelSA, newdata = testSA))
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
library(randomForest)
install.packages("randomForest")
library(randomForest)
modvowel <- randomForest(y ~ ., data = vowel.train)
order(varImp(modvowel), decreasing = T)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
library(randomForest)
modvowel <- randomForest(y ~ ., data = vowel.train)
order(varImp(modvowel), decreasing = T)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
library(randomForest)
modvowel <- randomForest(y ~ ., data = vowel.train)
order(varImp(modvowel), decreasing = T)
library(lubricate)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
mod_rf <- train(y ~ ., data = vowel.train, method = "rf")
mod_gbm <- train(y ~ ., data = vowel.train, method = "gbm")
?confusionMatrix
install.packages("lubridate")
install.packages("lubridate")
options(repos="https://CRAN.R-project.org")
install.packages("lubridate")
install.packages("lubridate")
install.packages("forecast")
install.packages("e1071")
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
str(vowel.train)
mod_rf=train(y,data=vowel.train,method='rf')
library(ElemStatLearn)
mod_rf=train(y,data=vowel.train,method='rf')
mod_rf=train(y~.,data=vowel.train,method='rf')
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
mod_rf <- train(y ~ ., data = vowel.train, method = "rf")
?train
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
?predict
install.packages("caret")
install.packages("AppliedPredictiveModeling")
mod_rf=train(y~.,data=vowel.train,method='rf')
install.packages("pgmm")
install.packages("rpart")
install.packages("gbm")
mod_rf=train(y~.,data=vowel.train,method='rf')
install.packages(c("BH", "class", "clipr", "codetools", "colorspace", "curl", "data.table", "evaluate", "foreign", "httr", "jsonlite", "lattice", "MASS", "Matrix", "mgcv", "modelr", "openssl", "pillar", "purrr", "Rcpp", "readr", "readxl", "rlang", "stringi", "stringr", "survival", "tibble", "tinytex"))
mod_rf=train(y~.,data=vowel.train,method='rf')
library(caret)
library(lattice)
library(ggplot2)
library(caret)
mod_rf=train(y~.,data=vowel.train,method='rf')
mod_gbm=train(y~.,data=vowel.train,method='gbm')
?predict
pred_rf=predict(mod_rf,vowel.train)
pred_gbm=predict(mod_gbm,vowel.train)
sum(pref_rf$y==vowel.test$y)/sum(vowel.test$y)
sum(pref_rf$y==vowel.test$y)/length(vowel.test$y)
sum(pred_rf$y==vowel.test$y)/length(vowel.test$y)
sum(pred_rf==vowel.test$y)/length(vowel.test$y)
sum(pred_rf==vowel.test$y)
confusionMatrix(pred_rf, vowel.test$y)$overall[1]
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
mod_rf <- train(y ~ ., data = vowel.train, method = "rf")
mod_gbm <- train(y ~ ., data = vowel.train, method = "gbm")
pred_rf <- predict(mod_rf, vowel.test)
pred_gbm <- predict(mod_gbm, vowel.test)
confusionMatrix(pred_gbm, vowel.test$y)$overall[1]
sum(pred_rf==vowel.test$y)/length(vowel.test$y)
confusionMatrix(pred_rf, vowel.test$y)$overall[1]
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
mod_rf=train(diagnosis~.,inTrain,method='rf')
mod_rf=train(diagnosis~.,data=inTrain,method='rf')
mod_rf=train(diagnosis~.,inTrain,method='rf')
str(inTrain)
mod_rf=train(diagnosis~.,data=training,method='rf')
mod_gbm=train(diagnosis~.,data=training,method='gbm')
mod_ida=train(diagnosis~.,data=training,method='ida')
mod_lda=train(diagnosis~.,data=training,method='lda')
pred_rf=predict(mod_rf,training)
pred_gbm=predict(mod_gbm,training)
pred_lda=predict(mod_lda,training)
comb=data.frame(pred_rf,pred_gbm,pred_lda)
comb=data.frame(pred_rf,pred_gbm,pred_lda,diagnosis=testing$diagnosis)
set.seed(62433)
mod_rf <- train(diagnosis ~ ., data = training, method = "rf")
mod_gbm <- train(diagnosis ~ ., data = training, method = "gbm")
mod_lda <- train(diagnosis ~ ., data = training, method = "lda")
pred_rf <- predict(mod_rf, testing)
pred_gbm <- predict(mod_gbm, testing)
pred_lda <- predict(mod_lda, testing)
predDF <- data.frame(pred_rf, pred_gbm, pred_lda, diagnosis = testing$diagnosis)
combModFit <- train(diagnosis ~ ., method = "rf", data = predDF)
combPred <- predict(combModFit, predDF)
confusionMatrix(pred_rf, testing$diagnosis)$overall[1]
confusionMatrix(pred_gbm, testing$diagnosis)$overall[1]
confusionMatrix(combPred, testing$diagnosis)$overall[1]
confusionMatrix(pred_gbm, testing$diagnosis)$overall[1]
confusionMatrix(pred_lda, testing$diagnosis)$overall[1]
knitr::opts_chunk$set(echo = TRUE,warning=F)
options(scipen = 1)
library(R.utils)
library(ggplot2)
library(plyr)
library(dplyr)
library(caret)
ModelRF2=randomForest(classe~.,data=Trainout)
library(randomForest)
ModelRF2=randomForest(classe~.,data=Trainout)
options(scipen = 1)
library(R.utils)
library(ggplot2)
library(lattice)
library(plyr)
library(dplyr)
library(caret)
library(randomForest)
library(rpart)
library(rpart.plot)
library(rattle)
if(!file.exists("pml-training.csv")) {
url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
download.file(url, "pml-training.csv")
}
if(!file.exists("pml-testing.csv")) {
url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(url, "pml-testing.csv")
}
TrainIn=read.csv("pml-training.csv",sep=",",na.strings=c("NA","#DIV/0!",""))
TestingIn=read.csv("pml-testing.csv",sep=",",na.strings=c("NA","#DIV/0!",""))
dim(TrainIn)
sum(is.na(TrainIn$var_accel_arm))
ColToRemove <- which(colSums(is.na(TrainIn)|TrainIn=="")>0.9*dim(TrainIn)[1])
Trainout2 <- TrainIn[,-ColToRemove]
Trainout <- Trainout2[,-c(1:7)]
ColToRemove <- which(colSums(is.na(TestingIn)|TestingIn=="")>0.9*dim(TestingIn)[1])
Testingout2 <- TestingIn[,-ColToRemove]
Testingout <- Testingout2[,-c(1:7)]
ModelRF2=randomForest(classe~.,data=TrainD)
preProc <- preProcess(Trainout[,1:52],method="pca",thresh=.95)
TrainPC<-predict(preProc,Trainout)
TestPC<-predict(preProc,Testingout)
set.seed(8888)
indTrain <- createDataPartition(Trainout$classe, p=0.75, list=FALSE)
TrainD <- Trainout[indTrain,]
TestD <- Trainout[-indTrain,]
ModelRF2=randomForest(classe~.,data=TrainD)
plot(ModelRF2$finalModel,main="Model error of Random forest model by number of trees")
print(modFitRF)
print(modFitRF2)
print(ModelRF2)
importance(modFitRF)
importance(ModelRF2)
View(TrainD)
setwd("~/GitHub/CourseProjectMachineLearning")
data <- read.csv("pml-training.csv")
colnames(data)
summary(data)
library(caret)
set.seed(1111)
train <- createDataPartition(y=data$classe,p=.70,list=F)
training <- data[train,]
testing <- data[-train,]
#exclude identifier, timestamp, and window data (they cannot be used for prediction)
Cl <- grep("name|timestamp|window|X", colnames(training), value=F)
trainingCl <- training[,-Cl]
#select variables with high (over 95%) missing data --> exclude them from the analysis
trainingCl[trainingCl==""] <- NA
NArate <- apply(trainingCl, 2, function(x) sum(is.na(x)))/nrow(trainingCl)
trainingCl <- trainingCl[!(NArate>0.95)]
summary(trainingCl)
preProc <- preProcess(trainingCl[,1:52],method="pca",thresh=.8) #12 components are required
preProc <- preProcess(trainingCl[,1:52],method="pca",thresh=.9) #18 components are required
preProc <- preProcess(trainingCl[,1:52],method="pca",thresh=.95) #25 components are required
preProc <- preProcess(trainingCl[,1:52],method="pca",pcaComp=25)
preProc$rotation
trainingPC <- predict(preProc,trainingCl[,1:52])
library(randomForest)
modFitRF <- randomForest(trainingCl$classe ~ .,   data=trainingPC, do.trace=F)
print(modFitRF) # view results
testingCl <- testing[,-Cl]
testingCl[testingCl==""] <- NA
NArate <- apply(testingCl, 2, function(x) sum(is.na(x)))/nrow(testingCl)
testingCl <- testingCl[!(NArate>0.95)]
testingPC <- predict(preProc,testingCl[,1:52])
confusionMatrix(testingCl$classe,predict(modFitRF,testingPC))
testdata <- read.csv("pml-testing.csv")
testdataCl <- testdata[,-Cl]
testdataCl[testdataCl==""] <- NA
NArate <- apply(testdataCl, 2, function(x) sum(is.na(x)))/nrow(testdataCl)
testdataCl <- testdataCl[!(NArate>0.95)]
testdataPC <- predict(preProc,testdataCl[,1:52])
testdataCl$classe <- predict(modFitRF,testdataPC)
testdataCl$classe
library(caret)
library(rattle)
TrainData=read.csv("pml-training.csv",sep=",",na.strings=c("NA","#DIV/0!",""))
TestData=read.csv("pml-testing.csv",sep=",",na.strings=c("NA","#DIV/0!",""))
indColToRemove <- which(colSums(is.na(TrainData) |TrainData=="")>0.9*dim(TrainData)[1])
TrainDataClean <- TrainData[,-indColToRemove]
TrainDataClean <- TrainDataClean[,-c(1:7)]
dim(TrainDataClean)
indColToRemove <- which(colSums(is.na(TestData) |TestData=="")>0.9*dim(TestData)[1])
TestDataClean <- TestData[,-indColToRemove]
TestDataClean <- TestDataClean[,-1]
dim(TestDataClean)
# Here we create a partition of the traning data set
set.seed(12345)
inTrain1 <- createDataPartition(TrainDataClean$classe, p=0.75, list=FALSE)
Train1 <- TrainDataClean[inTrain1,]
Test1 <- TrainDataClean[-inTrain1,]
dim(Train1)
trControl <- trainControl(method="cv", number=5)
model_CT <- train(classe~., data=Train1, method="rpart", trControl=trControl)
fancyRpartPlot(model_CT$finalModel)
trainpred <- predict(model_CT,newdata=Test1)
confMatCT <- confusionMatrix(Test1$classe,trainpred)
# display confusion matrix and model accuracy
confMatCT$table
confMatCT$overall[1]
model_RF <- train(classe~., data=Train1, method="rf", trControl=trControl, verbose=FALSE)
plot(model_RF,main="Accuracy of Random forest model by number of predictors")
trainpred <- predict(model_RF,newdata=Test1)
confMatRF <- confusionMatrix(Test1$classe,trainpred)
# display confusion matrix and model accuracy
confMatRF$table
plot(model_RF$finalModel,main="Model error of Random forest model by number of trees")
options(scipen = 1)
library(R.utils)
library(ggplot2)
library(lattice)
library(plyr)
library(dplyr)
library(caret)
library(randomForest)
library(rpart)
library(rpart.plot)
library(rattle)
if(!file.exists("pml-training.csv")) {
url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
download.file(url, "pml-training.csv")
}
if(!file.exists("pml-testing.csv")) {
url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(url, "pml-testing.csv")
}
TrainIn=read.csv("pml-training.csv",sep=",",na.strings=c("NA","#DIV/0!",""))
TestingIn=read.csv("pml-testing.csv",sep=",",na.strings=c("NA","#DIV/0!",""))
dim(TrainIn)
sum(is.na(TrainIn$var_accel_arm))
ColToRemove <- which(colSums(is.na(TrainIn)|TrainIn=="")>0.9*dim(TrainIn)[1])
Trainout2 <- TrainIn[,-ColToRemove]
Trainout <- Trainout2[,-c(1:7)]
ColToRemove <- which(colSums(is.na(TestingIn)|TestingIn=="")>0.9*dim(TestingIn)[1])
Testingout2 <- TestingIn[,-ColToRemove]
Testingout <- Testingout2[,-c(1:7)]
str(TestDataClean)
Testingout2 <- TestingIn[,-ColToRemove]
Testingout <- Testingout2[,-1]
Testingout$set.seed(12345)
indTrain <- createDataPartition(Trainout$classe, p=0.75, list=FALSE)
TrainD <- Trainout[indTrain,]
TestD <- Trainout[-indTrain,]
trControl=trainControl(method='CV',number=5)
ModelCT=train(classe~.,data=TrainD,method='rpart',trControl=trControl)
PredCT=predict(ModelCT,newdata=TestD)
confCT=confusionMatrix(TestD$classe,PredCT)
confCT$overall[1]
fancyRpartPlot(model_CT$finalModel)
fancyRpartPlot(ModelCT$finalModel)
confCT$table
set.seed(12345)
indTrain <- createDataPartition(Trainout$classe, p=0.75, list=FALSE)
TrainD <- Trainout[indTrain,]
TestD <- Trainout[-indTrain,]
print(ModelCT)
trControl=trainControl(method='cv',number=5)
ModelCT=train(classe~.,data=TrainD,method='rpart',trControl=trControl)
PredCT=predict(ModelCT,newdata=TestD)
confCT=confusionMatrix(TestD$classe,PredCT)
confCT$overall[1]
ModelRF <- train(classe~., data=TrainD, method="rf")
PredRF=predict(ModelRF,newdata=TestD)
ModelRF2=randomForest(classe~.,data=TrainD)
confRF <- confusionMatrix(TestD$classe,PredRF)
confRF$overall[1]
plot(ModelRF$finalModel,main="Model error of Random forest model by number of trees")
MostImpVars <- varImp(ModelRF)
MostImpVars
confRF$overall[1]
ModelGBM <- train(classe~., data=TrainD, method="gbm",trControl=trControl, verbose=FALSE)
PredGBM <- predict(ModelGBM,newdata=TestD)
confGBM <- confusionMatrix(TestD$classe,PredGBM)
confGBM$overall[1]
FinalTestPred <- predict(model_RF,newdata=Testingout)
FinalTestPred
FinalTestPred <- predict(ModelRF,newdata=Testingout)
FinalTestPred
